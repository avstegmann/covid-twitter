{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Corona-Tweet Analysis\n",
    "### Samuel Heinz, Alexander von Stegmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Import Libraries and load training set and data set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from collections import Counter\n",
    "from nltk import NaiveBayesClassifier\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'neutral': 398, 'acknowledged': 276, 'not related': 180, 'opposed': 146})\n"
     ]
    }
   ],
   "source": [
    "td = pd.read_csv('training_data.csv', sep='|', lineterminator='\\n')\n",
    "\n",
    "# prepare full corpus\n",
    "LABELS = {'acknowledged', 'opposed', 'neutral', 'not related'}\n",
    "full_corpus = [\n",
    "    (text, label)\n",
    "    for label in LABELS\n",
    "    for text in td.text.loc[td[label] == 1].values.tolist()\n",
    "]\n",
    "\n",
    "lbl_dist = Counter(label for _, label in full_corpus)\n",
    "print(lbl_dist)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NORMALIZE_RULES = {\n",
    "    \"it's\": \"it is\",\n",
    "    \"itâ€™s\": \"it is\",\n",
    "    \"n't\": \" not\",\n",
    "    \"nâ€™t\": \" not\",\n",
    "    \"'m\": \" am\",\n",
    "    \"â€™m\": \" am\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"â€™ve\": \" have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"â€™re\": \" are\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"â€™ll\": \" will\",\n",
    "    \"&amp;\": \"and\",\n",
    "    \"&gt;\": \"\"\n",
    "}\n",
    "\n",
    "def normalize(text):\n",
    "    for old, new in NORMALIZE_RULES.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = set(punctuation)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # text = re.sub('(?<=\\d),(?=\\d)', '', text)\n",
    "    text = re.sub('([,\\d]*\\d,\\d[,\\d]*)', 'num', text)\n",
    "    text = re.sub('(\\d{,3}[.]*\\d{2,}%)', 'perc', text)\n",
    "    for p in PUNCTUATION:\n",
    "        text = text.replace(p, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Define Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Lemmatizing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "LEMMA = WordNetLemmatizer()\n",
    "\n",
    "def convert_pos_tag(tag):\n",
    "\n",
    "    converted = wordnet.NOUN\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        converted = wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        converted = wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        converted = wordnet.ADV\n",
    "\n",
    "    return converted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Stemming"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "STEMMER = SnowballStemmer(language='english')\n",
    "\n",
    "def stem(tokens):\n",
    "    return [STEMMER.stem(token) for token in tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes a text (or tweet) and applies following rules. Afterwards the text is split into tokens that\n",
    "    are lemmatized and stemmed\n",
    "    :param text: full str of a text\n",
    "    :return: list of tokens\n",
    "    \"\"\"\n",
    "    # 1.1 Lower case everything\n",
    "    lower = text.lower()\n",
    "    # 1.2 Normalize & clear punctuation\n",
    "    normalized = normalize(lower)\n",
    "    no_punct = remove_punctuation(normalized)\n",
    "    # 1.3 \"de-emojize\"\n",
    "    demojized = emoji.demojize(no_punct, delimiters=[' ',' '])\n",
    "    # 1.4 Create tokens and drop Stopwords\n",
    "    tokens = [token\n",
    "              for token in demojized.split()\n",
    "              if token not in STOPWORDS]\n",
    "\n",
    "    # 2. POS-Tagging\n",
    "    with_pos = nltk.pos_tag(tokens)\n",
    "\n",
    "    # 3.1 Conversion of pos tags for lemmatizer\n",
    "    with_converted_pos = [(token, convert_pos_tag(tag)) for token, tag in with_pos]\n",
    "\n",
    "    # 3.2 Lemmatize\n",
    "    lemmatized_tokens = [LEMMA.lemmatize(token, pos=tag) for token, tag in with_converted_pos]\n",
    "\n",
    "    # 4. Stemming\n",
    "    stemmed_tokens = [STEMMER.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "    return stemmed_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "howclean = td.text.values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "howclean2 = [tokenize(text) for text in howclean]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : Organizers of today's #SickOutBC are not ruling out another protest before #Christmas.   Langley mom Miranda Tracy (@mjt8080) says many parents --and teachers-- do not trust public health claims #COVID19 transmission rates among children are low.    #bced #bcpoli @NEWS1130\n",
      "Tokens : ['organ', 'today', 'sickoutbc', 'rule', 'anoth', 'protest', 'christma', 'langley', 'mom', 'miranda', 'traci', 'mjt8080', 'say', 'mani', 'parent', 'teacher', 'trust', 'public', 'health', 'claim', 'covid19', 'transmiss', 'rate', 'among', 'child', 'low', 'bced', 'bcpoli', 'news1130'] \n",
      "\n",
      "Input : So questionnn the people administering the vaccine are they getting it they self??? #vaccine  #COVID19\n",
      "Tokens : ['questionnn', 'peopl', 'administ', 'vaccin', 'get', 'self', 'vaccin', 'covid19'] \n",
      "\n",
      "Input : President #Trump to sign decree that prioritises American citizens getting access to #coronavirus vaccine before other nations.\n",
      "Tokens : ['presid', 'trump', 'sign', 'decre', 'prioritis', 'american', 'citizen', 'get', 'access', 'coronavirus', 'vaccin', 'nation'] \n",
      "\n",
      "Input : Hey @realDonaldTrump  I need a #Pardon too, a #rentpardpn for the last six months of rent because of how you fucked up the #COVID19 pandemic response!\n",
      "Tokens : ['hey', 'realdonaldtrump', 'need', 'pardon', 'rentpardpn', 'last', 'six', 'month', 'rent', 'fuck', 'covid19', 'pandem', 'respons'] \n",
      "\n",
      "Input : Wow they got transformers 7 coming out in the summer we better hope this #COVID19 will be going away\n",
      "Tokens : ['wow', 'get', 'transform', '7', 'come', 'summer', 'well', 'hope', 'covid19', 'go', 'away'] \n",
      "\n",
      "Input : \"Vaccine will be available in the coming weeks,\" says @DrBonnieHenry  #COVID19BC  #COVID19\n",
      "Tokens : ['vaccin', 'avail', 'come', 'week', 'say', 'drbonniehenri', 'covid19bc', 'covid19'] \n",
      "\n",
      "Input : Just because politicians are human (and therefore hypocrites) does not necessarily mean their recommendations should not be followed #justsayin' #COVID19\n",
      "Tokens : ['politician', 'human', 'therefor', 'hypocrit', 'necessarili', 'mean', 'recommend', 'follow', 'justsayin', 'covid19'] \n",
      "\n",
      "Input : You want the truth about #COVID19 ? Do some research. Google search your area population numbers, and then google your areaâ€™s COVID-19 cases...youâ€™ll be amazed at how many more cases there are than people...ðŸ¤¨ðŸ˜³ #MAGA\n",
      "Tokens : ['want', 'truth', 'covid19', 'research', 'googl', 'search', 'area', 'popul', 'number', 'googl', 'area', 'covid', '19', 'case', 'amaz', 'mani', 'case', 'peopl', 'face_with_raised_eyebrow', 'flushed_fac', 'maga'] \n",
      "\n",
      "Input : so the #corona has done century\n",
      "Tokens : ['corona', 'do', 'centuri'] \n",
      "\n",
      "Input : In Indiana, a Hoosier died of COVID-19 every 41 minutes and 22 seconds in the last 24 hours. #COVID19\n",
      "Tokens : ['indiana', 'hoosier', 'die', 'covid', '19', 'everi', '41', 'minut', '22', 'second', 'last', '24', 'hour', 'covid19'] \n",
      "\n",
      "Input : Hey teachers: Thank you for all of your efforts in 2020! All students and teachers are working daily to survive a global pandemic  ðŸ˜· #COVID\n",
      "Tokens : ['hey', 'teacher', 'thank', 'effort', '2020', 'student', 'teacher', 'work', 'daili', 'surviv', 'global', 'pandem', 'face_with_medical_mask', 'covid'] \n",
      "\n",
      "Input : Cases: 64,174,915 (+77,764)  Deaths: 1,486,681 (+0)  Recovered: 41,249,718 (+48,794)  Active Cases: 22,925,197 (35.72%) Completed Cases: 42,736,399 (66.59%) Mortality Rate: 2.32% (No Change) Case Fatality Rate: 3.48% (No Change)  #COVID19 #Coronavirus #StayHome\n",
      "Tokens : ['case', 'num', 'num', 'death', 'num', '0', 'recov', 'num', 'num', 'activ', 'case', 'num', 'perc', 'complet', 'case', 'num', 'perc', 'mortal', 'rate', 'perc', 'chang', 'case', 'fatal', 'rate', 'perc', 'chang', 'covid19', 'coronavirus', 'stayhom'] \n",
      "\n",
      "Input : In the U.S. 14,700,000+ cases and 285,000+ deaths.  *worldometer   #COVID  #TrumpVirusCatastrophe #BlameTrump\n",
      "Tokens : ['u', 'num', 'case', 'num', 'death', 'worldomet', 'covid', 'trumpviruscatastroph', 'blametrump'] \n",
      "\n",
      "Input : One American is dying every minute from #COVID, 24/7. #TrumpLiedPeopleDied @dougducey\n",
      "Tokens : ['one', 'american', 'die', 'everi', 'minut', 'covid', '24', '7', 'trumpliedpeopledi', 'dougducey'] \n",
      "\n",
      "Input : Nobody dies from anything anymore but #covid and nobody questions this? #COVIDIOTS\n",
      "Tokens : ['nobodi', 'die', 'anyth', 'anymor', 'covid', 'nobodi', 'question', 'covidiot'] \n",
      "\n",
      "Input : #BREAKING: #Ontario reports 1,873 new #Covid19 cases today: Toronto 522 Peel 436 York 185 Hamilton 109  17 deaths reported &amp;  1,918 more resolved cases.\n",
      "Tokens : ['break', 'ontario', 'report', 'num', 'new', 'covid19', 'case', 'today', 'toronto', '522', 'peel', '436', 'york', '185', 'hamilton', '109', '17', 'death', 'report', 'num', 'resolv', 'case'] \n",
      "\n",
      "Input : Today, Johns Hopkins reports 297,697 covid deaths in the US. If it were a single city, the covid graveyard would now pass Pittsburgh, PA to become the 68th largest metro market in the country. #COVID19 #COVID @PittsburghPG @Pittsburgh\n",
      "Tokens : ['today', 'john', 'hopkin', 'report', 'num', 'covid', 'death', 'u', 'singl', 'citi', 'covid', 'graveyard', 'would', 'pass', 'pittsburgh', 'pa', 'becom', '68th', 'larg', 'metro', 'market', 'countri', 'covid19', 'covid', 'pittsburghpg', 'pittsburgh'] \n",
      "\n",
      "Input : 35K new #COVID19 cases in the UK today ðŸ‘€ðŸ‘€ðŸ‘€\n",
      "Tokens : ['35k', 'new', 'covid19', 'case', 'uk', 'today', 'eye', 'eye', 'eye'] \n",
      "\n",
      "Input : So #Dartmouth surplus is now  closed and wasauctioned off today... There will be many more gone before life finds its new normal. #COVID\n",
      "Tokens : ['dartmouth', 'surplus', 'close', 'wasauct', 'today', 'mani', 'go', 'life', 'find', 'new', 'normal', 'covid'] \n",
      "\n",
      "Input : Just occurred to me that most people actually think they are going to have a choice regarding this #COVID19 vaccine ...\n",
      "Tokens : ['occur', 'peopl', 'actual', 'think', 'go', 'choic', 'regard', 'covid19', 'vaccin'] \n",
      "\n",
      "Input : #BREAKING: #UK approves #Pfizer-#BioNTech vaccine, set to be rolled out next week. #Coronavirus #CoronaVirusUpdates #CoronavirusVaccine\n",
      "Tokens : ['break', 'uk', 'approv', 'pfizer', 'biontech', 'vaccin', 'set', 'roll', 'next', 'week', 'coronavirus', 'coronavirusupd', 'coronavirusvaccin'] \n",
      "\n",
      "Input : Shaadi's have become the breeding groud for #Corona\n",
      "Tokens : ['shaadi', 'becom', 'breed', 'groud', 'corona'] \n",
      "\n",
      "Input : According to a survey by @Alignable ,50% Of B2C Business Owners reported they are At Risk Of Failing!  Congressional Leaders,  clock is ticking,  sit at the table and don't leave till you have a deal,  #COVID19 #coronavirus #stimuluspackage #SmallBusinesses #Entrepreneurship\n",
      "Tokens : ['accord', 'survey', 'align', 'perc', 'b2c', 'busi', 'owner', 'report', 'risk', 'fail', 'congression', 'leader', 'clock', 'tick', 'sit', 'tabl', 'leav', 'till', 'deal', 'covid19', 'coronavirus', 'stimuluspackag', 'smallbusi', 'entrepreneurship'] \n",
      "\n",
      "Input : Was there a cure for influenza that big pharma didn't share with us???? #COVID19\n",
      "Tokens : ['cure', 'influenza', 'big', 'pharma', 'share', 'u', 'covid19'] \n",
      "\n",
      "Input : Itâ€™s a good day to be a science geek ðŸ‘ðŸ½ #COVID19 #vaccine\n",
      "Tokens : ['good', 'day', 'scienc', 'geek', 'clapping_hands_medium_skin_ton', 'covid19', 'vaccin'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,25):\n",
    "    print(f\"Input : {howclean[i]}\")\n",
    "    print(f\"Tokens : {howclean2[i]} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Feature Extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "def extract_features(tokens, vocabulary):\n",
    "    features = {}\n",
    "\n",
    "    token_count = Counter(tokens)\n",
    "\n",
    "    for vocab_token in vocabulary:\n",
    "        features[f'amount({vocab_token})'] = token_count[vocab_token]\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4  Dividing Training Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def train_test_split(corpus, amount=0.8):\n",
    "    split_index = int(len(corpus) * amount)\n",
    "\n",
    "    shuffled = random.sample(corpus, len(corpus))\n",
    "\n",
    "    train = shuffled[:split_index]\n",
    "    test = shuffled[split_index:]\n",
    "\n",
    "    return train, test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "def build_vocabulary(corpus, num_words=300):\n",
    "    full_count = Counter(token for tokens, _ in corpus for token in tokens)\n",
    "    return sorted([token for token, _ in full_count.most_common(num_words)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2 Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 Split the data set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set has 899 items, the test set 101\n"
     ]
    },
    {
     "data": {
      "text/plain": "(Counter({'not related': 162,\n          'neutral': 358,\n          'acknowledged': 248,\n          'opposed': 131}),\n Counter({'not related': 18,\n          'neutral': 40,\n          'acknowledged': 28,\n          'opposed': 15}))"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = []\n",
    "test_set = []\n",
    "for label in LABELS:\n",
    "    train_tweets, test_tweets = train_test_split([tweet for tweet in full_corpus if tweet[1] == label], 0.9)\n",
    "    train_set.extend(train_tweets)\n",
    "    test_set.extend(test_tweets)\n",
    "\n",
    "print(f\"The train set has {len(train_set)} items, the test set {len(test_set)}\")\n",
    "\n",
    "Counter(label for _ , label in train_set), Counter(label for _ , label in test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2 Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_preprocessed = [(tokenize(tweets), labels) for tweets, labels in tqdm(train_set, total=len(train_set), desc='Preprocessing')]\n",
    "test_set_preprocessed = [(tokenize(tweets), labels) for tweets, labels in tqdm(test_set, total=len(test_set), desc='Preprocessing')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3 Build vocabulary (only nltk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "TRAIN_VOCABULARY = build_vocabulary(train_set_preprocessed, 300)\n",
    "TEST_VOCABULARY = build_vocabulary(test_set_preprocessed, 300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.4 Feature extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = [(extract_features(tokens, TRAIN_VOCABULARY), label) for tokens, label in tqdm(train_set_preprocessed, total=len(train_set_preprocessed), desc='Preprocessing')]\n",
    "test_data = [(extract_features(tokens, TEST_VOCABULARY), label) for tokens, label in tqdm(test_set_preprocessed, total=len(test_set_preprocessed), desc='Preprocessing')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.5 Train classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "#nltk\n",
    "nb = NaiveBayesClassifier.train(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            amount(life) = 1              not re : neutra =     13.9 : 1.0\n",
      "       amount(wearamask) = 1              acknow : neutra =     13.0 : 1.0\n",
      "          amount(govern) = 1              oppose : acknow =     11.9 : 1.0\n",
      "            amount(wear) = 1              acknow : neutra =     11.3 : 1.0\n",
      " amount(realdonaldtrump) = 1              acknow : neutra =     11.1 : 1.0\n",
      "           amount(elect) = 1              not re : neutra =     11.0 : 1.0\n",
      "            amount(even) = 1              not re : neutra =     11.0 : 1.0\n",
      "            amount(mask) = 1              acknow : neutra =      9.6 : 1.0\n",
      "             amount(new) = 1              neutra : oppose =      9.5 : 1.0\n",
      "            amount(want) = 1              oppose : neutra =      8.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#nltk\n",
    "nb.show_most_informative_features(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5495049504950495"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk\n",
    "nltk.classify.accuracy(nb, test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#nltk\n",
    "predictions = nb.classify_many([feature for feature, _ in test_data])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#nltk\n",
    "gold = [label for _, label in test_data]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-Precision: 0.55\n",
      "Micro-Recall   : 0.55\n",
      "Micro-FScore   : 0.55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Micro Average\n",
    "\n",
    "tp, fp, fn = 0, 0, 0\n",
    "\n",
    "for predicted, correct in zip(predictions, gold):\n",
    "    for label in LABELS:\n",
    "        if correct == label:\n",
    "            if predicted == label:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted == label:\n",
    "                fp += 1\n",
    "            # We don't care about TN for precision/recall\n",
    "\n",
    "micro_precision = tp / (tp + fp)\n",
    "micro_recall = tp / (tp + fn)\n",
    "micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "\n",
    "print(f\"\"\"\n",
    "Micro-Precision: {micro_precision:.2f}\n",
    "Micro-Recall   : {micro_recall:.2f}\n",
    "Micro-FScore   : {micro_fscore:.2f}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per Label:\n",
      "\tnot related   : 0.45\n",
      "\tneutral       : 0.73\n",
      "\tacknowledged  : 0.47\n",
      "\topposed       : 0.44\n",
      "\n",
      "Recall per Label:\n",
      "\tnot related   : 0.58\n",
      "\tneutral       : 0.64\n",
      "\tacknowledged  : 0.48\n",
      "\topposed       : 0.40\n",
      "\n",
      "F-Score per Label:\n",
      "\tnot related   : 0.51\n",
      "\tneutral       : 0.68\n",
      "\tacknowledged  : 0.47\n",
      "\topposed       : 0.42\n",
      "\n",
      "\n",
      "Macro-Precision: 0.52\n",
      "Macro-Recall   : 0.53\n",
      "Macro-FScore   : 0.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Macro Average\n",
    "\n",
    "precisions, recalls, fscores = {}, {}, {} # as dictionary so, we store it by _label_\n",
    "\n",
    "for label in LABELS:\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for predicted, correct in zip(predictions, gold):\n",
    "        if correct == label:\n",
    "            if predicted == label:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted == label:\n",
    "                fp += 1\n",
    "    p = tp / (tp + fp)\n",
    "    r = tp / (tp + fn)\n",
    "    f = (2 * p * r) / (p + r)\n",
    "\n",
    "    precisions[label] = p\n",
    "    recalls[label] = r\n",
    "    fscores[label] = f\n",
    "\n",
    "\n",
    "print(f\"Precision per Label:\")\n",
    "print('\\n'.join(['\\t' + f'{label:<14}: {value:.2f}' for label, value in precisions.items()]))\n",
    "print()\n",
    "\n",
    "print(f\"Recall per Label:\")\n",
    "print('\\n'.join(['\\t' + f'{label:<14}: {value:.2f}' for label, value in recalls.items()]))\n",
    "print()\n",
    "\n",
    "print(f\"F-Score per Label:\")\n",
    "print('\\n'.join(['\\t' + f'{label:<14}: {value:.2f}' for label, value in fscores.items()]))\n",
    "print()\n",
    "\n",
    "macro_precision = sum(precisions.values()) / len(precisions)\n",
    "macro_recall = sum(recalls.values()) / len(recalls)\n",
    "macro_fscore = sum(fscores.values()) / len(fscores)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "Macro-Precision: {macro_precision:.2f}\n",
    "Macro-Recall   : {macro_recall:.2f}\n",
    "Macro-FScore   : {macro_fscore:.2f}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nexample = td.text[7]\\nprint(f\"Input : \\'{example}\\'\")\\nprint(f\"Tokens: {tokenization(example)}\")\\n'"
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "example = td.text[7]\n",
    "print(f\"Input : '{example}'\")\n",
    "print(f\"Tokens: {tokenization(example)}\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nexample = \"#COVID19 why are those that work closely with COVID patients (a highly contagious vaccine) a priority if they had been doing work since the beginning? Wouldnâ€™t it be better to use for those with higher health risks? Isnâ€™t the current system they are using be working?\"\\ntokens = tokenization(example)\\nfeatures = extract_features(tokens)\\noutput = nb.classify(features)\\n\\nprint(f\"The classifier predicts: {output}\")\\n'"
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "example = \"#COVID19 why are those that work closely with COVID patients (a highly contagious vaccine) a priority if they had been doing work since the beginning? Wouldnâ€™t it be better to use for those with higher health risks? Isnâ€™t the current system they are using be working?\"\n",
    "tokens = tokenization(example)\n",
    "features = extract_features(tokens)\n",
    "output = nb.classify(features)\n",
    "\n",
    "print(f\"The classifier predicts: {output}\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}